
<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN" "http://www.w3c.org/TR/1999/REC-html401-19991224/loose.dtd">
<html xml:lang="en" xmlns="http://www.w3.org/1999/xhtml" lang="en"><head>
  <title>Scalable Neural Indoor Scene Rendering</title>
<meta http-equiv="Content-Type" content="text/html; charset=windows-1252">

<meta property="og:image" content=""/>
<meta property="og:title" content="Scalable Neural Indoor Scene Rendering" />
<meta name="Description" content="Scalable Neural Indoor Scene Rendering" />
<meta name="Keywords" content="Scalable, Neural rendering, Indoor Scene, Rendering, NeRF, neural radiance, image-based rendering, IBR, Neural radiance field, radiance field" />
<meta name="google-site-verification" content="EHqRPVRs5I7MJ6ut-LWwjYlHivbFEgUcSH2eMfAqbo0" />

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-53682931-1', 'auto');
  ga('send', 'pageview');

</script>

<script type="text/javascript">
// redefining default features
var _POPUP_FEATURES = 'width=500,height=300,resizable=1,scrollbars=1,titlebar=1,status=1';

</script>


<script src="js/slides.js"></script>


<link media="all" href="glab.css" type="text/css" rel="StyleSheet">
<style type="text/css" media="all">
IMG {
	PADDING-RIGHT: 0px;
	PADDING-LEFT: 0px;
	FLOAT: right;
	PADDING-BOTTOM: 0px;
	PADDING-TOP: 0px
}
#primarycontent {
	MARGIN-LEFT: auto; ; WIDTH: expression(document.body.clientWidth >
960? "960px": "auto" ); MARGIN-RIGHT: auto; TEXT-ALIGN: left; max-width:
960px }
BODY {
	TEXT-ALIGN: center
}
</style>


<body onload='changevideo("videos/salon.mp4",3)'>

  <!-- <div class="my_logo">
    <img alt="" src="images/logo.png" width=35%>
</div> -->

<div id="primarycontent">

  <div class="uni_div">
    <img src="images/university.png" width="60%" >
  </div>

<br>
<br>
<br>
<center><h1>Scalable Neural Indoor Scene Rendering</h1></center>


  
<center><h5>SIGGRAPH 2022 (Journal track) </h5></center>


<br>
<center><h4>
  <a href="https://xchaowu.github.io/"><u>Xiuchao Wu</u></a><sup>1*</sup>&nbsp;&nbsp;&nbsp;&nbsp;
  <a href="https://superxjm.github.io/"><u>Jiamin Xu</u></a><sup>1*</sup>&nbsp;&nbsp;&nbsp;&nbsp;
  <a href="https://zzh2000.github.io/"><u>Zihan Zhu</u></a><sup>1</sup>&nbsp;&nbsp;&nbsp;
  <a href="http://www.cad.zju.edu.cn/home/bao/"><u>Hujun Bao</u></a><sup>1</sup>&nbsp;&nbsp;&nbsp;
  <a href="https://www.cs.utexas.edu/~huangqx/"><u>Qixing Huang</u></a><sup>2</sup>&nbsp;&nbsp;&nbsp;
  <a href="https://jamestompkin.com/"><u>James Tompkin</u></a><sup>3</sup>&nbsp;&nbsp;&nbsp;
  <a href="http://www.cad.zju.edu.cn/home/weiweixu/weiweixu_en.htm"><u>Weiwei Xu</u></a><sup>1</sup></h4>
</center>
<br>
<center><h4><sup>*</sup>joint first authors</h4></center>
<!-- <center>
  <img src="images/university.png" width=50%>
</center> -->
<br>

<!-- <div class="uni_div">
  <img src="images/university.png" width="60%" >
</div> -->




<!-- <br><br><br> -->
<center><h4><sup>1</sup>Zhejiang University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <sup>2</sup>University of Texas at Austin&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
  <sup>3</sup>Brown University&nbsp;&nbsp;</center>



<center>
<div class="button_icons">

  <div class="button_my">
  <a href="snisr.pdf">
    <img style="PADDING-TOP: 12px;" alt="" src="images/paper_logo2.png" width=35%>
    <div class="my_txt"><span><a>Paper</a></span></div>
  </a>
</div>

<div class="button_my">
  <a href="https://www.youtube.com/watch?v=hn-IVZZ-_lM">
    <img style="PADDING-TOP: 15px;" alt="" src="images/video_logo.png" width=65%>
    <div class="my_txt"><span><a>Video</a></span></div>
  </a>
</div>

<div class="button_my">
  <a href="https://github.com/XchaoWu/Scalable-Neural-Indoor-Scene-Rendering">
    <img style="PADDING-TOP: 12px;" alt="" src="images/code_logo.png" width=63%>
    <div class="my_txt"><span><a>Code</a></span></div>
  </a>
</div>

<div class="button_my">
  <a href="https://docs.google.com/presentation/d/1HNUrycwS31bU5flg8Kr5pZvmOBKqb6Rz/edit?usp=sharing&ouid=107369102250640073628&rtpof=true&sd=true">
    <img style="PADDING-TOP: 15px;" alt="" src="images/ppt_logo2.png" width=60%>
    <div class="my_txt"><span><a>PPT</a></span></div>
  </a>
</div>

</div>
</center>


<br><br><br><br><br>


<img class="teaser" src="images/teaser.png"> 
<br><br><br><br><br><br><br><br><br><br>
<br><br><br><br><br><br><br>
<center><h2>Abstract</h2></center>
<div style="font-size:14px; text-align:justify;"><p>We propose a scalable neural scene reconstruction and rendering method to
  support distributed training and interactive rendering of large indoor scenes.
  Our representation is based on tiles and a separation of view-independent
  appearance (diffuse color and shading) and view-dependent appearance
  (specular highlights, reflections), each of which predicted by lower-capacity
  MLPs. After assigning MPLs per tile, our scheme allows tile MLPs to be
  trained in parallel and still represent complex reflections through a two-pass
  training strategy. This is enabled by a background sampling strategy that can
  augment tile information from a proxy global mesh geometry and tolerate
  typical errors from reconstructed proxy geometry. Further, we design a
  two-MLP based representation at each tile to leverage the phenomena that
  view-dependent surface effects can be attributed to a reflected virtual light at
  the total ray distance to the source. This lets us handle sparse samplings of
  the input scene where reflection highlights do not always appear consistently
  in input images. We show interactive free-viewpoint rendering results from
  five scenes. One of them covers areas of more than 100 &#13217. Experimental
  results show that our method produces higher-quality renderings than a
  single large-capacity MLP and other recent baseline methods.</p></div>


<center><h2>Rendering Results</h2></center>
<div class="container">

  <center>
    <video id="ref_demo" width="960" height="540" autoplay loop controls>
      <source id="source" src="">
    </video>
  </center>
  <!-- Next and previous buttons -->
  <!-- <a class="prev" onclick="plusSlides(-1)">&#10094;</a>
  <a class="next" onclick="plusSlides(1)">&#10095;</a> -->

  <!-- Image text -->
  <div class="caption-container">
    <p id="caption"></p>
  </div>

  <!-- Thumbnail images -->
  <center>
  <div class="row">
    <div class="column">
      <img class="demo cursor" draggable="false" src="images/1.png" style="width:100%" onclick='changevideo("videos/livingroom.mp4", 1)' alt="LivingRoom2 (1280 x 720)">
      <div class="lab"><span>LivingRoom2</span></div>
    </div>
    <div class="column">
      <img class="demo cursor" draggable="false" src="images/2.png" style="width:100%" onclick='changevideo("videos/coffee.mp4", 2)' alt="Coffee Shop (1280 x 720)">
      <div class="lab"><span>Coffee Shop</span></div>
    </div>
    <div class="column">
      <img class="demo cursor" draggable="false" src="images/4.png" style="width:100%" onclick='changevideo("videos/salon.mp4", 3)' alt="LivingRoom1 (900 x 610)">
      <div class="lab"><span>LivingRoom1</span></div>
    </div>
    <div class="column">
      <img class="demo cursor" draggable="false" src="images/7.png" style="width:100%" onclick='changevideo("videos/Bar.mp4", 4)' alt="Bar (1280 x 720)">
      <div class="lab"><span>Bar</span></div>
    </div>
    <div class="column">
      <img class="demo cursor" draggable="false" src="images/8.png" style="width:100%" onclick='changevideo("videos/sofa.mp4", 5)' alt="Sofa (1280 x 720)">
      <div class="lab"><span>Sofa</span></div>
    </div>
  </div>
</center>
</div>


<!-- <center><h2>Full Video</h2></center>
<center>
<table align="center" border="0" cellspacing="0" cellpadding="0">
  <iframe width="960" height="540" src="https://www.youtube.com/embed/hn-IVZZ-_lM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
  </iframe>
</table>
</center> -->

<!-- <br><br><br>
<hr> -->
<!-- <img class="teaser" src="images/pipeline.png">  -->






<!-- <br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<br><br><br> -->
<center><h2>Method</h2></center>
<div style="font-size:14px; text-align:justify;"><p>We create tiles over the volumetric scene and optimize per-tile MLPs. Each tile has two MLPs: 1) The surface MLP that encodes density and view independent color, which is later stored in an octree for fast rendering. 2) The reflection MLP that encodes view-dependent effects 
  like highlights using virtual points underneath the surface at the ray distance of 
  the reflected light. Color outputs from both paths are combined in the final rendering.</p></div>
  <img class="teaser" src="images/pipeline.png"> 

<br>
<br>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br>

<center><h2>Explainer Video</h2></center>
<center>
  <table align="center" border="0" cellspacing="0" cellpadding="0">
    <iframe width="960" height="540" src="https://www.youtube.com/embed/8xK9JZkxHRk" title="YouTube video player1" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
  </table>
  </center> 

<!-- <hr> -->
<br>
<br>
<center><h2>Distributed Training</h2></center>
<center><p>Our scheme allows tile MLPs to be trained in parallel.</p></center>

<center>
  <video src="videos/distributed_loop.mp4" height="265" autoplay loop muted playsinline></video>
  <video src="videos/dt_1.mp4" height="265" autoplay loop muted playsinline></video>
</center>

<br>
<center><h2>Interactive Rendering</h2></center>
<center><p>The rendering time for a frame of resolution <math> <mn> 1280 </mn> <mo> &#x00D7; <!-- multiplication sign --> </mo> <mn> 720 </mn> </math> is 50ms on average.</p></center>
<center>
  <video src="videos/online_others.mp4" width="960"  autoplay loop muted playsinline></video>
  </center>
<center>
  <video src="videos/online.mp4" width="960" autoplay loop muted playsinline></video>
</center>
<!-- <center>
  <video src="videos/online.mp4" width="560" height="318" autoplay loop muted></video>
  <video src="videos/distributed_loop.mp4" width="560" height="318" autoplay loop muted></video>
  <video src="videos/online_others.mp4" width="1080" height="540" autoplay loop muted></video>
</center> -->

<center><h2>Comparisons</h2></center>
<center><p>Our method shows improved rendering results for specular
  reflection and temporal coherence over baselines</p></center>
<center>
  <video src="videos/more_com.mp4" width="960" height="540" controls></video>
</center>

<br>
<center><h2>Extrapolation</h2></center>
<center><p>We test the case when novel viewpoints are far from captured views.</p></center>

<center>
  <video src="videos/FarRender.mp4" width="960" autoplay loop muted playsinline></video>
</center>
<br>

<center><h2>Simple Editing</h2></center>
<center>
  <video src="videos/moving_tiles.mp4" width="960" autoplay loop muted playsinline></video>
</center>
<center>
  <video src="videos/copy3.mp4" width="960" autoplay loop muted playsinline></video>
</center>
<br>



<br>
<br>
<hr>
<center><h2>Full Video</h2></center>
<center>
  <table align="center" border="0" cellspacing="0" cellpadding="0">
    <iframe width="960" height="540" src="https://www.youtube.com/embed/hn-IVZZ-_lM" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen>
    </iframe>
  </table>
  </center> 

<center><h2>BibTex</h2></center>
<pre style="width:960px;overflow-x:auto">
  <code>
    @article{wu2022snisr,
      title={Scalable Neural Indoor Scene Rendering},
      author={Wu, Xiuchao and Xu, Jiamin and Zhu, Zihan and Bao, Hujun and Huang, Qixing and Tompkin, James and Xu, Weiwei},
      journal={ACM Transactions on Graphics (TOG)},
      year={2022}
    }
  </code>
</pre>

<center><h2>Acknowledgements</h2></center>
<div class='ttt1'>
<center>
  <p>Supported by Information Technology Center and State Key Lab of CAD&CG, Zhejiang University. Special thanks to <u><a href="https://cv.wangchi.art">Chi Wang</a></u> for offering the data collection site.</p>
</center>

</div>

<!-- <center>
  <video src="videos/demo_12.mp4" width="960" height="540" controls></video>
</center> -->
<!-- <br>
<br>
<br>
<center>
  <video id="ref_demo" style="width:80%" autoplay loop controls>
    <source id="source" src="videos/livingroom.mp4">
  </video>
</center>
<br>
<br>
<br> -->

<!-- Container for the image gallery -->

<hr>

<br>
<footer>
  <div style="width: 100%;" class="footer">
    <center>
      <p>
        Data Living Room1 and Sofa are from <u><a href="https://repo-sam.inria.fr/fungraph/deep-indoor-relight/">Inria</a></u> 
      </p>
      <p>Data Living Room2 is from paper <a href="https://superxjm.github.io/source_files/ReflectiveIBR.pdf"><u>Scalable Image-based Indoor Scene Rendering with Reflections</u></a></p>
    </center>
  </div>
</footer>

</body></html>
<!-- 
<div class="demo">    <div class="item">
  <h2 name="Results"> Static relighting interpolation</h2>
  <video muted autoplay loop controls ><source src='videos/livingroom.mp4' type='video/mp4'></video>
  
  <ul id="lightSliderres0">
  <li data-thumb="images/video_icon.png">
  <a name="results"></a>
  <h2 name="Results">Path n&deg1: View synthesis</h2>
  <video muted autoplay loop controls ><source src='videos/Bar.mp4' type='video/mp4'></video></li>
  <li data-thumb="images/video_icon.png">
  <a name="results"></a>
  <h2 name="Results">Path n&deg1: Relit view synthesis n&deg1</h2>
  <video muted autoplay loop controls ><source src='videos/coffee.mp4' type='video/mp4'></video></li>
  <li data-thumb="images/video_icon.png">
  <a name="results"></a>
  <h2 name="Results">Path n&deg1: Relit view synthesis n&deg2</h2>
  <video muted autoplay loop controls ><source src='videos/sofa.mp4' type='video/mp4'></video></li>
  </ul>
  <script type="text/javascript">
  $(document).ready(function() {$("#lightSliderres0").lightSlider({gallery:true,item:1,thumbItem:3,slideMargin: 0,loop:true,adaptiveHeight:true,});});
  </script>
</div></div> -->

  
<!-- <h2>Tile Groups</h2>
<center><a href="images/sofa.gif">
  <img src="images/sofa.gif" width="256"> 
  </a></center>
<center><a href="images/floor.gif">
  <img src="images/floor.gif" width="256"> 
  </a></center>
<center><a href="images/light.gif">
  <img src="images/light.gif" width="256"> 
  </a></center>
<center><a href="images/table.gif">
  <img src="images/table.gif" width="256"> 
  </a></center>
<br><br><br><br><br><br><br><br><br><br><br><br><br><br><br>
<hr/> -->
<!-- <h2>Video</h2>
<table align="center" border="0" cellspacing="0" cellpadding="0">
    <tr>
    <td align="center" valign="middle">

	
		<iframe width="640" height="360" src="https://www.youtube.com/embed/qtBecWttXH4?rel=0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
   </td>
    </tr>
</table>
<p>&nbsp;</p>
<hr/> -->

<!-- <h2>Code & Data</h2>
<table align="left" border="0" cellspacing="0" cellpadding="0">
  <tr>
  <td align="left" valign="middle">

    <a href="https://i.cs.hku.hk/~nlchen/curvefusion/">
      <image width="100" height="100" src="data_ico.png">
    </a>
  
 </td>

 <td align="left" valign="middle">
  <a href="https://drive.google.com/drive/folders/1jr5jsR8j4lofcoNNN68HlGvXhVck4mgj">
    <image width="100" height="100" src="data_ico.png">
  </a>
 </td>
  </tr>
  <tr>
    <td align="middle" valign="middle">
  
  
    <a href="https://i.cs.hku.hk/~nlchen/curvefusion/" width="100" height="100">Code</a>
   </td>
  
   <td align="middle" valign="middle">
  
  
    <a href="https://drive.google.com/drive/folders/1jr5jsR8j4lofcoNNN68HlGvXhVck4mgj" width="100" height="100">Data</a>
   </td>
    </tr>
</table> -->
<p>&nbsp;</p>

<!-- 
<h2>Citation</h2>
				<div class="section bibtex">
					<pre>
@article{curvefusion,
                author = {Liu, Lingjie and Chen, Nenglun and Ceylan, Duygu and Theobalt, Christian and Wang, Wenping and Mitra, Niloy J.},
                title = {Scalable Neural Indoor Scene Rendering},
                journal = {ACM Trans. Graph.},
                issue_date = {November 2018},
                volume = {37},
                number = {6},
                month = dec,
                year = {2018},
                issn = {0730-0301},
                pages = {218:1--218:12},
                articleno = {218},
                numpages = {12},
                url = {http://doi.acm.org/10.1145/3272127.3275097},
                doi = {10.1145/3272127.3275097},
                acmid = {3275097},
                publisher = {ACM},
                address = {New York, NY, USA},
               }
				</div>
<br>
<br>
<br> -->

<!-- <h2>Acknowledgement</h2>
<p>We thank our reviewers for their invaluable comments. We thank Hui Huang, Amy Tabb and Zheng Wang for their great help with the testing and validation of our work. We also thank Jiatao Gu, Cheng Lin, Zhiming Cui, Runnan Chen, Maria Lam, Paul Guerrero, Elizabeth Schildge for their help. This work was partially funded by the ERC Starting Grant SmartGeometry (StG-2013-335373), the Research Grant Council of Hong Kong (GRF 17210718), ERC Starting Grant 
CapReal (335545), a Google Faculty award, a UCL visiting student program, and gifts from Adobe.</p>
<br>
<br>
<br>
<br> -->
<!--<h2>Citation</h2>
<p></p>-->
<!-- 
<div style="display:none">


